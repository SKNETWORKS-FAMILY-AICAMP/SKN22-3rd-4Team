"""
SEC EDGAR 10-K ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ë° ê´€ê³„ ì •ë³´ ì¶”ì¶œ

Supabaseì— ì €ì¥ëœ ê¸°ì—… ë˜ëŠ” S&P 500 ì „ì²´ ê¸°ì—…ì˜ 10-K ì „ì²´ ë¬¸ì„œë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ ,
ê¸°ì—… ê°„ ê´€ê³„ ì •ë³´(ê³µê¸‰ì—…ì²´, ê³ ê°, ê²½ìŸì‚¬, ìíšŒì‚¬)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""

import os
import re
import sys
import json
import time
import requests
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from bs4 import BeautifulSoup
import pandas as pd
from dotenv import load_dotenv
from supabase import create_client, Client

load_dotenv()

# SEC API ì„¤ì •
SEC_BASE_URL = "https://www.sec.gov"
SEC_EDGAR_URL = "https://data.sec.gov"

# Supabase ì„¤ì •
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")


def get_companies_from_supabase() -> Tuple[List[str], Dict[str, str]]:
    """Supabaseì—ì„œ ê¸°ì—… í‹°ì»¤ì™€ CIK ëª©ë¡ ê°€ì ¸ì˜¤ê¸°

    Returns:
        Tuple[List[str], Dict[str, str]]: (í‹°ì»¤ ëª©ë¡, {í‹°ì»¤: CIK} ë§¤í•‘)
    """
    try:
        if not SUPABASE_URL or not SUPABASE_KEY:
            print("âš ï¸  Supabase ì„¤ì • ì—†ìŒ, ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.")
            return get_sp500_from_wikipedia(), {}

        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        result = supabase.table("companies").select("ticker, cik").execute()

        tickers = []
        db_cik_map = {}
        for row in result.data:
            if row["ticker"]:
                tickers.append(row["ticker"])
                if row.get("cik"):
                    db_cik_map[row["ticker"]] = row["cik"]

        print(
            f"âœ… Supabaseì—ì„œ {len(tickers)}ê°œ ê¸°ì—… ë¡œë“œë¨ (CIK: {len(db_cik_map)}ê°œ)"
        )
        return tickers, db_cik_map
    except Exception as e:
        print(f"âš ï¸  Supabase ì—°ê²° ì‹¤íŒ¨: {e}, ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.")
        return get_sp500_from_wikipedia(), {}


def get_sp500_from_wikipedia() -> List[str]:
    """ìœ„í‚¤í”¼ë””ì•„ì—ì„œ S&P 500 ê¸°ì—… ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"

    try:
        tables = pd.read_html(url)
        sp500_table = tables[0]
        tickers = sp500_table["Symbol"].str.replace(".", "-", regex=False).tolist()
        print(f"âœ… ìœ„í‚¤í”¼ë””ì•„ì—ì„œ S&P 500 {len(tickers)}ê°œ ê¸°ì—… ë¡œë“œë¨")
        return tickers
    except Exception as e:
        print(f"âŒ S&P 500 ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []


def get_company_list(source: str = "supabase") -> Tuple[List[str], Dict[str, str]]:
    """ê¸°ì—… ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (source: 'supabase' ë˜ëŠ” 'wikipedia')

    Returns:
        Tuple[List[str], Dict[str, str]]: (í‹°ì»¤ ëª©ë¡, DBì—ì„œ ê°€ì ¸ì˜¨ CIK ë§¤í•‘)
    """
    if source == "wikipedia":
        return get_sp500_from_wikipedia(), {}
    else:
        return get_companies_from_supabase()


# ê´€ê³„ í‚¤ì›Œë“œ íŒ¨í„´
RELATIONSHIP_PATTERNS = {
    "supplier": [
        r"(?:our |the )?(?:primary |major |key |principal )?supplier[s]?(?:\s+include|\s+are|\s+such as)?[\s:]+([A-Z][A-Za-z\s&,]+)",
        r"(?:we |the company )?(?:source[s]? |purchase[s]? |procure[s]? )(?:from|through)\s+([A-Z][A-Za-z\s&,]+)",
        r"(?:manufactured |produced |supplied )(?:by|from)\s+([A-Z][A-Za-z\s&,]+)",
    ],
    "customer": [
        r"(?:our |the )?(?:largest |major |key |principal )?customer[s]?(?:\s+include|\s+are)?[\s:]+([A-Z][A-Za-z\s&,]+)",
        r"(?:we |the company )?(?:sell[s]? |provide[s]? )(?:to|services? to)\s+([A-Z][A-Za-z\s&,]+)",
        r"revenue[s]? from\s+([A-Z][A-Za-z\s&,]+)",
    ],
    "competitor": [
        r"(?:our |the )?(?:primary |major |key )?competitor[s]?(?:\s+include|\s+are)?[\s:]+([A-Z][A-Za-z\s&,]+)",
        r"(?:we )?compete[s]? (?:with|against)\s+([A-Z][A-Za-z\s&,]+)",
        r"competition from\s+([A-Z][A-Za-z\s&,]+)",
    ],
    "subsidiary": [
        r"(?:our )?(?:wholly[- ]owned )?subsidiar(?:y|ies)(?:\s+include)?[\s:]+([A-Z][A-Za-z\s&,]+)",
        r"(?:we )?(?:own[s]?|acquired)\s+([A-Z][A-Za-z\s&,]+)",
    ],
    "partner": [
        r"(?:our |the )?(?:strategic )?partner(?:ship)?[s]?(?:\s+with|\s+include)?[\s:]+([A-Z][A-Za-z\s&,]+)",
        r"(?:joint venture|collaboration|alliance)\s+(?:with|between)\s+([A-Z][A-Za-z\s&,]+)",
    ],
}

# ì•Œë ¤ì§„ ê¸°ì—…ëª… ëª©ë¡ (ë§¤ì¹­ ì •í™•ë„ í–¥ìƒìš©)
KNOWN_COMPANIES = set(
    [
        "Apple",
        "Microsoft",
        "Google",
        "Alphabet",
        "Amazon",
        "Meta",
        "Facebook",
        "NVIDIA",
        "Tesla",
        "TSMC",
        "Taiwan Semiconductor",
        "Broadcom",
        "Qualcomm",
        "Intel",
        "AMD",
        "Samsung",
        "SK Hynix",
        "Micron",
        "Texas Instruments",
        "JPMorgan",
        "Goldman Sachs",
        "Morgan Stanley",
        "Bank of America",
        "Wells Fargo",
        "Visa",
        "Mastercard",
        "American Express",
        "PayPal",
        "Johnson & Johnson",
        "Pfizer",
        "Merck",
        "AbbVie",
        "Bristol-Myers",
        "UnitedHealth",
        "CVS",
        "Cigna",
        "Anthem",
        "Humana",
        "Walmart",
        "Target",
        "Costco",
        "Amazon",
        "Home Depot",
        "Coca-Cola",
        "PepsiCo",
        "McDonald's",
        "Starbucks",
        "Nike",
        "Disney",
        "ExxonMobil",
        "Chevron",
        "ConocoPhillips",
        "Shell",
        "BP",
        "AT&T",
        "Verizon",
        "T-Mobile",
        "Comcast",
        "Charter",
        "Boeing",
        "Lockheed Martin",
        "Raytheon",
        "General Dynamics",
        "Northrop Grumman",
        "Caterpillar",
        "Deere",
        "3M",
        "Honeywell",
        "General Electric",
        "Oracle",
        "Salesforce",
        "Adobe",
        "SAP",
        "ServiceNow",
        "Intuit",
        "Foxconn",
        "Hon Hai",
        "Pegatron",
        "Wistron",
        "Luxshare",
    ]
)


def get_user_agent():
    """SEC API ìš”ì²­ì— í•„ìš”í•œ User-Agent"""
    email = os.getenv("SEC_API_USER_AGENT", "researcher@university.edu")
    return f"Mozilla/5.0 (compatible; ResearchBot/1.0; +mailto:{email})"


def get_company_cik_map() -> Dict[str, dict]:
    """í‹°ì»¤-CIK ë§¤í•‘ ì¡°íšŒ"""
    headers = {"User-Agent": get_user_agent(), "Accept": "application/json"}

    url = "https://www.sec.gov/files/company_tickers.json"
    response = requests.get(url, headers=headers, timeout=30)
    response.raise_for_status()
    data = response.json()

    cik_map = {}
    for item in data.values():
        ticker = item.get("ticker", "").upper()
        cik = str(item.get("cik_str", "")).zfill(10)
        title = item.get("title", "")
        cik_map[ticker] = {"cik": cik, "title": title}

    return cik_map


def get_10k_filing_url(cik: str, headers: dict) -> Optional[Tuple[str, str]]:
    """ê°€ì¥ ìµœê·¼ 10-K íŒŒì¼ë§ URL ì¡°íšŒ"""
    submissions_url = f"{SEC_EDGAR_URL}/submissions/CIK{cik}.json"

    try:
        response = requests.get(submissions_url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()

        recent_filings = data.get("filings", {}).get("recent", {})
        forms = recent_filings.get("form", [])
        accessions = recent_filings.get("accessionNumber", [])
        primary_docs = recent_filings.get("primaryDocument", [])
        filing_dates = recent_filings.get("filingDate", [])

        for i, form in enumerate(forms):
            if form == "10-K":
                accession = accessions[i].replace("-", "")
                primary_doc = primary_docs[i]
                filing_date = filing_dates[i]

                doc_url = f"{SEC_BASE_URL}/Archives/edgar/data/{cik.lstrip('0')}/{accession}/{primary_doc}"
                return doc_url, filing_date

        return None, None

    except Exception as e:
        print(f"      10-K ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return None, None


def download_10k_document(url: str, headers: dict) -> Optional[str]:
    """10-K ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ"""
    try:
        response = requests.get(url, headers=headers, timeout=60)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"      ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None


def extract_text_from_html(html_content: str) -> str:
    """HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    soup = BeautifulSoup(html_content, "html.parser")

    # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ì œê±°
    for tag in soup(["script", "style", "meta", "link"]):
        tag.decompose()

    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text = soup.get_text(separator=" ", strip=True)

    # ì •ë¦¬
    text = re.sub(r"\s+", " ", text)

    return text


def extract_sections(text: str) -> Dict[str, str]:
    """10-K ì£¼ìš” ì„¹ì…˜ ì¶”ì¶œ"""
    sections = {}

    # Item 1 - Business
    item1_pattern = r"(?:ITEM\s*1\.?\s*[-â€“â€”]?\s*BUSINESS)(.*?)(?:ITEM\s*1A|ITEM\s*2)"
    match = re.search(item1_pattern, text, re.IGNORECASE | re.DOTALL)
    if match:
        sections["business"] = match.group(1)[:50000]  # ìµœëŒ€ 50K ë¬¸ì

    # Item 1A - Risk Factors
    item1a_pattern = (
        r"(?:ITEM\s*1A\.?\s*[-â€“â€”]?\s*RISK\s*FACTORS)(.*?)(?:ITEM\s*1B|ITEM\s*2)"
    )
    match = re.search(item1a_pattern, text, re.IGNORECASE | re.DOTALL)
    if match:
        sections["risk_factors"] = match.group(1)[:50000]

    # Item 7 - MD&A
    item7_pattern = r"(?:ITEM\s*7\.?\s*[-â€“â€”]?\s*MANAGEMENT)(.*?)(?:ITEM\s*7A|ITEM\s*8)"
    match = re.search(item7_pattern, text, re.IGNORECASE | re.DOTALL)
    if match:
        sections["mda"] = match.group(1)[:50000]

    return sections


def extract_relationships(text: str, source_company: str) -> List[Dict]:
    """í…ìŠ¤íŠ¸ì—ì„œ ê¸°ì—… ê´€ê³„ ì¶”ì¶œ"""
    relationships = []

    for rel_type, patterns in RELATIONSHIP_PATTERNS.items():
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)

            for match in matches:
                # ë§¤ì¹˜ëœ ê¸°ì—…ëª… ì •ë¦¬
                companies = clean_company_names(match)

                for company in companies:
                    if company and len(company) > 2:
                        relationships.append(
                            {
                                "source": source_company,
                                "target": company,
                                "type": rel_type,
                            }
                        )

    # ì•Œë ¤ì§„ ê¸°ì—…ëª… ì§ì ‘ ê²€ìƒ‰
    for known_company in KNOWN_COMPANIES:
        if known_company.lower() in text.lower() and known_company != source_company:
            # ì´ë¯¸ ì¶”ê°€ëœ ê´€ê³„ì¸ì§€ í™•ì¸
            existing = any(
                r["target"].lower() == known_company.lower() for r in relationships
            )
            if not existing:
                relationships.append(
                    {
                        "source": source_company,
                        "target": known_company,
                        "type": "mentioned",
                    }
                )

    return relationships


def clean_company_names(text: str) -> List[str]:
    """ê¸°ì—…ëª… ì •ë¦¬ ë° ë¶„ë¦¬"""
    # ì½¤ë§ˆ, and ë“±ìœ¼ë¡œ ë¶„ë¦¬
    companies = re.split(r"[,;]|\band\b|\bor\b", text)

    cleaned = []
    for company in companies:
        company = company.strip()
        # ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
        company = re.sub(
            r"\b(Inc|Corp|Corporation|LLC|Ltd|Company|Co)\b\.?",
            "",
            company,
            flags=re.IGNORECASE,
        )
        company = company.strip(" .,")

        # ìµœì†Œ ê¸¸ì´ í™•ì¸
        if len(company) > 2 and not company.lower() in ["the", "our", "their", "such"]:
            cleaned.append(company)

    return cleaned


def save_document(ticker: str, content: str, sections: Dict, output_dir: Path):
    """ë¬¸ì„œ ë° ì„¹ì…˜ ì €ì¥"""
    ticker_dir = output_dir / ticker
    ticker_dir.mkdir(parents=True, exist_ok=True)

    # ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥
    with open(ticker_dir / "full_text.txt", "w", encoding="utf-8") as f:
        f.write(content)

    # ì„¹ì…˜ë³„ ì €ì¥
    for section_name, section_text in sections.items():
        with open(ticker_dir / f"{section_name}.txt", "w", encoding="utf-8") as f:
            f.write(section_text)


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import argparse

    parser = argparse.ArgumentParser(description="SEC 10-K ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ë° ê´€ê³„ ì¶”ì¶œ")
    parser.add_argument(
        "--source",
        choices=["supabase", "wikipedia"],
        default="supabase",
        help="ê¸°ì—… ëª©ë¡ ì†ŒìŠ¤ (supabase: DBì— ì €ì¥ëœ ê¸°ì—…, wikipedia: S&P 500 ì „ì²´)",
    )
    parser.add_argument(
        "--ticker",
        type=str,
        default=None,
        help="íŠ¹ì • í‹°ì»¤ë§Œ ì²˜ë¦¬ (ì˜ˆ: --ticker MRSH)",
    )
    parser.add_argument(
        "--skip-existing",
        action="store_true",
        default=True,
        help="ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ë¬¸ì„œ ìŠ¤í‚µ (ê¸°ë³¸ê°’: True)",
    )
    args = parser.parse_args()

    print("=" * 70)
    print("ğŸ“¥ SEC 10-K ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ë° ê´€ê³„ ì¶”ì¶œ")
    if args.ticker:
        print(f"   í‹°ì»¤: {args.ticker}")
    else:
        print(f"   ì†ŒìŠ¤: {args.source}")
    print("=" * 70)

    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = Path("data/10k_documents")
    output_dir.mkdir(parents=True, exist_ok=True)

    # ê¸°ì—… ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    if args.ticker:
        # íŠ¹ì • í‹°ì»¤ë§Œ ì²˜ë¦¬
        companies = [args.ticker.upper()]
        _, db_cik_map = get_company_list(args.source)
    else:
        # ì „ì²´ ê¸°ì—… ì²˜ë¦¬
        companies, db_cik_map = get_company_list(args.source)

    if not companies:
        print("âŒ ê¸°ì—… ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # SECì—ì„œ CIK ë§¤í•‘ ë¡œë“œ
    print("\nğŸ“‹ SECì—ì„œ CIK ë§¤í•‘ ë¡œë“œ ì¤‘...")
    sec_cik_map = get_company_cik_map()
    print(f"   SEC: {len(sec_cik_map)}ê°œ, DB: {len(db_cik_map)}ê°œ CIK ë¡œë“œë¨")

    headers = {
        "User-Agent": get_user_agent(),
        "Accept-Encoding": "gzip, deflate",
    }

    # ê´€ê³„ ë°ì´í„° ì €ì¥ìš©
    all_relationships = []
    processed_companies = []

    total_companies = len(companies)
    print(f"\nğŸ“Š {total_companies}ê°œ ê¸°ì—… 10-K ì²˜ë¦¬ ì¤‘...\n")

    for i, ticker in enumerate(companies, 1):
        lookup_ticker = ticker.replace("-", "")

        # SECì—ì„œ ë¨¼ì € ì°¾ê³ , ì—†ìœ¼ë©´ DBì—ì„œ ì°¾ê¸° (fallback)
        company_info = sec_cik_map.get(ticker) or sec_cik_map.get(lookup_ticker)

        # SECì—ì„œ ëª» ì°¾ìœ¼ë©´ DBì˜ CIK ì‚¬ìš©
        if not company_info and (ticker in db_cik_map or lookup_ticker in db_cik_map):
            db_cik = db_cik_map.get(ticker) or db_cik_map.get(lookup_ticker)
            if db_cik:
                company_info = {
                    "cik": db_cik.zfill(10) if len(db_cik) < 10 else db_cik,
                    "title": ticker,
                }
                print(
                    f"  [{i:3d}/{total_companies}] {ticker}: DB CIK ì‚¬ìš© ({db_cik})",
                    end="",
                    flush=True,
                )

        if not company_info:
            print(f"  [{i:3d}/{total_companies}] {ticker}: âŒ CIK ì—†ìŒ")
            continue

        cik = company_info["cik"]
        company_name = company_info["title"]

        print(
            f"  [{i:3d}/{total_companies}] {ticker}: {company_name[:35]:<35}",
            end="",
            flush=True,
        )

        try:
            # 10-K URL ì¡°íšŒ
            doc_url, filing_date = get_10k_filing_url(cik, headers)

            if not doc_url:
                print(" âš ï¸ 10-K ì—†ìŒ")
                continue

            # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
            ticker_dir = output_dir / ticker
            if args.skip_existing and (ticker_dir / "full_text.txt").exists():
                print(" â­ï¸ ì´ë¯¸ ì¡´ì¬")

                # ê´€ê³„ë§Œ ì¶”ì¶œ
                with open(ticker_dir / "full_text.txt", "r", encoding="utf-8") as f:
                    content = f.read()

                relationships = extract_relationships(content[:100000], company_name)
                all_relationships.extend(relationships)
                processed_companies.append(
                    {
                        "ticker": ticker,
                        "name": company_name,
                        "filing_date": filing_date or "unknown",
                        "relationships": len(relationships),
                    }
                )
                continue

            # ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ
            html_content = download_10k_document(doc_url, headers)

            if not html_content:
                print(" âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                continue

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_content = extract_text_from_html(html_content)

            # ì„¹ì…˜ ì¶”ì¶œ
            sections = extract_sections(text_content)

            # ì €ì¥
            save_document(ticker, text_content, sections, output_dir)

            # ê´€ê³„ ì¶”ì¶œ
            relationships = extract_relationships(text_content[:100000], company_name)
            all_relationships.extend(relationships)

            processed_companies.append(
                {
                    "ticker": ticker,
                    "name": company_name,
                    "filing_date": filing_date,
                    "relationships": len(relationships),
                }
            )

            print(f" âœ… {len(relationships)}ê°œ ê´€ê³„")

            # Rate limiting
            time.sleep(0.2)

        except Exception as e:
            print(f" âŒ ì˜¤ë¥˜: {str(e)[:30]}")

    # ê²°ê³¼ ì €ì¥
    print("\n" + "=" * 70)
    print("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")

    # ê´€ê³„ ë°ì´í„° ì €ì¥
    relationships_df = pd.DataFrame(all_relationships)
    relationships_df.to_csv(output_dir / "relationships.csv", index=False)
    print(f"   ê´€ê³„ ë°ì´í„°: {len(all_relationships)}ê°œ â†’ relationships.csv")

    # JSONìœ¼ë¡œë„ ì €ì¥
    with open(output_dir / "relationships.json", "w", encoding="utf-8") as f:
        json.dump(all_relationships, f, ensure_ascii=False, indent=2)

    # ì²˜ë¦¬ëœ ê¸°ì—… ëª©ë¡
    processed_df = pd.DataFrame(processed_companies)
    processed_df.to_csv(output_dir / "processed_companies.csv", index=False)
    print(f"   ì²˜ë¦¬ëœ ê¸°ì—…: {len(processed_companies)}ê°œ â†’ processed_companies.csv")

    # ìš”ì•½
    print("\n" + "=" * 70)
    print("ğŸ“Š ì¶”ì¶œëœ ê´€ê³„ ìš”ì•½:")
    if not relationships_df.empty:
        rel_summary = relationships_df["type"].value_counts()
        for rel_type, count in rel_summary.items():
            print(f"   {rel_type}: {count}ê°œ")

    print("\nâœ… ì™„ë£Œ!")
    print(f"   ì €ì¥ ìœ„ì¹˜: {output_dir}")
    print("=" * 70)
    print(
        "\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: upload_relationships_to_supabase.py ì‹¤í–‰í•˜ì—¬ ê´€ê³„ ë°ì´í„°ë¥¼ DBì— ì—…ë¡œë“œí•˜ì„¸ìš”."
    )


if __name__ == "__main__":
    main()
