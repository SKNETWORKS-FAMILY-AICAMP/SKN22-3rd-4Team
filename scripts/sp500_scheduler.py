"""
S&P 500 ê¸°ì—… ì •ë³´ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬
ë§¤ì¼ ìƒˆë²½ 5ì‹œ(KST)ì— S&P 500 ê¸°ì—… ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python scripts/sp500_scheduler.py          # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ë§¤ì¼ 05:00 KST ì‹¤í–‰)
    python scripts/sp500_scheduler.py --test   # ì¦‰ì‹œ 1íšŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    python scripts/sp500_scheduler.py --help   # ë„ì›€ë§
"""

import os
import sys
import time
import logging
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional

# Heavy imports moved to functions
# import pandas as pd
# import requests
# import yfinance as yf
# import pytz

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv

load_dotenv()

# ë¡œê·¸ ì„¤ì •
LOG_DIR = project_root / "data" / "logs"
LOG_DIR.mkdir(parents=True, exist_ok=True)


def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    log_file = LOG_DIR / f"scheduler_{datetime.now().strftime('%Y%m%d')}.log"

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler(),
        ],
    )
    return logging.getLogger(__name__)


logger = setup_logging()

# Slack ì•Œë¦¼ ê¸°ëŠ¥ ì œê±°ë¨ (User request)
# SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")

# def send_slack_notification(message: str, is_error: bool = False):
#     pass


def get_sp500_tickers() -> List[str]:
    """ìœ„í‚¤í”¼ë””ì•„ì—ì„œ S&P 500 í‹°ì»¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    import requests
    import pandas as pd

    url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"

    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()

        tables = pd.read_html(response.text)
        sp500_table = tables[0]

        # í‹°ì»¤ ëª©ë¡ ì¶”ì¶œ (. -> - ë³€í™˜)
        tickers = sp500_table["Symbol"].str.replace(".", "-", regex=False).tolist()

        logger.info(f"âœ… S&P 500 ê¸°ì—… {len(tickers)}ê°œ í‹°ì»¤ ë¡œë“œ ì™„ë£Œ")
        return tickers

    except Exception as e:
        logger.error(f"âŒ S&P 500 í‹°ì»¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []


def fetch_company_info(ticker: str) -> Optional[Dict]:
    """yfinanceë¡œ ê¸°ì—… ì •ë³´ ìˆ˜ì§‘ (ë‹¨ì¼ ì‹œë„)"""
    import yfinance as yf

    try:
        stock = yf.Ticker(ticker)
        info = stock.info

        if not info or "symbol" not in info:
            return None

        return {
            "ticker": ticker,
            "company_name": info.get("longName") or info.get("shortName", ""),
            "sector": info.get("sector", ""),
            "industry": info.get("industry", ""),
            "market_cap": info.get("marketCap", 0),
            "current_price": info.get("currentPrice")
            or info.get("regularMarketPrice", 0),
            "previous_close": info.get("previousClose", 0),
            "52_week_high": info.get("fiftyTwoWeekHigh", 0),
            "52_week_low": info.get("fiftyTwoWeekLow", 0),
            "pe_ratio": info.get("trailingPE"),
            "eps": info.get("trailingEps"),
            "dividend_yield": info.get("dividendYield"),
            "beta": info.get("beta"),
            "volume": info.get("volume", 0),
            "avg_volume": info.get("averageVolume", 0),
            "website": info.get("website", ""),
            "description": (
                info.get("longBusinessSummary", "")[:500]
                if info.get("longBusinessSummary")
                else ""
            ),
            "country": info.get("country", ""),
            "exchange": info.get("exchange", ""),
            "currency": info.get("currency", "USD"),
            "collected_at": datetime.now(pytz.timezone("Asia/Seoul")).isoformat(),
        }

    except Exception as e:
        raise e


def fetch_company_info_with_retry(ticker: str) -> Optional[Dict]:
    """yfinanceë¡œ ê¸°ì—… ì •ë³´ ìˆ˜ì§‘ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    for attempt in range(MAX_RETRIES):
        try:
            result = fetch_company_info(ticker)
            if result:
                return result
            # ê²°ê³¼ê°€ Noneì´ë©´ ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ (ìœ íš¨í•˜ì§€ ì•Šì€ í‹°ì»¤)
            return None
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                delay = RETRY_DELAY_BASE * (2**attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                logger.warning(
                    f"  âš ï¸ {ticker}: ì¬ì‹œë„ {attempt + 1}/{MAX_RETRIES} ({delay}ì´ˆ í›„)"
                )
                time.sleep(delay)
            else:
                logger.warning(f"  âŒ {ticker}: ìµœì¢… ì‹¤íŒ¨ - {e}")
                return None
    return None


def save_to_supabase(data: List[Dict]) -> int:
    """Supabaseì— ë°ì´í„° ì €ì¥ (upsert)"""
    import pytz

    try:
        from src.data.supabase_client import SupabaseClient

        client = SupabaseClient.get_client()

        success_count = 0

        for company in data:
            try:
                # company í…Œì´ë¸”ì— upsert (updated_at í¬í•¨)
                upsert_data = {
                    "ticker": company["ticker"],
                    "company_name": company["company_name"],
                    "sector": company["sector"],
                    "industry": company["industry"],
                    "market_cap": company["market_cap"],
                    "website": company["website"],
                    "exchange": company["exchange"],
                    "updated_at": datetime.now(pytz.timezone("Asia/Seoul")).isoformat(),
                }

                # upsert (ticker ê¸°ì¤€)
                result = (
                    client.table("companies")
                    .upsert(upsert_data, on_conflict="ticker")
                    .execute()
                )

                if result.data:
                    success_count += 1

            except Exception as e:
                logger.warning(f"  âš ï¸ {company['ticker']} DB ì €ì¥ ì‹¤íŒ¨: {e}")

        return success_count

    except Exception as e:
        logger.error(f"âŒ Supabase ì—°ê²° ì˜¤ë¥˜: {e}")
        return 0


def save_to_csv(data: List[Dict], output_dir: Path = None):
    """ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥ (ë°±ì—…ìš©)"""
    import pandas as pd

    if not data:
        return

    output_dir = output_dir or (project_root / "data" / "processed")
    output_dir.mkdir(parents=True, exist_ok=True)

    df = pd.DataFrame(data)

    # ë‚ ì§œë³„ íŒŒì¼ëª…
    date_str = datetime.now().strftime("%Y%m%d")
    csv_file = output_dir / f"sp500_data_{date_str}.csv"

    df.to_csv(csv_file, index=False, encoding="utf-8-sig")
    logger.info(f"ğŸ’¾ CSV ì €ì¥ë¨: {csv_file}")

    # JSONë„ ì €ì¥
    json_file = output_dir / f"sp500_data_{date_str}.json"
    df.to_json(json_file, orient="records", force_ascii=False, indent=2)
    logger.info(f"ğŸ’¾ JSON ì €ì¥ë¨: {json_file}")


def collect_sp500_data():
    """S&P 500 ê¸°ì—… ì •ë³´ ìˆ˜ì§‘ ë©”ì¸ í•¨ìˆ˜"""
    import pytz

    kst = pytz.timezone("Asia/Seoul")
    start_time = datetime.now(kst)

    logger.info("=" * 60)
    logger.info(
        f"ğŸš€ S&P 500 ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {start_time.strftime('%Y-%m-%d %H:%M:%S KST')}"
    )
    logger.info("=" * 60)

    # 1. S&P 500 í‹°ì»¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    tickers = get_sp500_tickers()
    if not tickers:
        logger.error("âŒ í‹°ì»¤ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ê° ê¸°ì—… ì •ë³´ ìˆ˜ì§‘
    all_data = []
    success_count = 0
    failed_tickers = []  # ì‹¤íŒ¨í•œ í‹°ì»¤ ëª©ë¡

    logger.info(f"\nğŸ“Š {len(tickers)}ê°œ ê¸°ì—… ì •ë³´ ìˆ˜ì§‘ ì¤‘...\n")

    for i, ticker in enumerate(tickers, 1):
        info = fetch_company_info_with_retry(ticker)

        if info:
            all_data.append(info)
            success_count += 1
            if i % 50 == 0:  # 50ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
                logger.info(
                    f"  [{i:3d}/{len(tickers)}] ì§„í–‰ ì¤‘... (ì„±ê³µ: {success_count})"
                )
        else:
            failed_tickers.append(ticker)

        # Rate limit ë°©ì§€ (0.2ì´ˆ ë”œë ˆì´)
        time.sleep(0.2)

    # 2-1. ì‹¤íŒ¨í•œ í‹°ì»¤ë“¤ ë§ˆì§€ë§‰ ì¬ì‹œë„ (5ë¶„ í›„)
    if failed_tickers:
        logger.info(f"\nğŸ”„ ì‹¤íŒ¨í•œ {len(failed_tickers)}ê°œ í‹°ì»¤ ì¬ì‹œë„ ì¤‘...")
        time.sleep(5)  # 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„

        retry_success = 0
        still_failed = []

        for ticker in failed_tickers:
            info = fetch_company_info_with_retry(ticker)
            if info:
                all_data.append(info)
                retry_success += 1
                logger.info(f"  âœ… {ticker}: ì¬ì‹œë„ ì„±ê³µ")
            else:
                still_failed.append(ticker)
            time.sleep(0.3)

        success_count += retry_success
        logger.info(
            f"  ğŸ”„ ì¬ì‹œë„ ê²°ê³¼: ì„±ê³µ {retry_success}ê°œ, ìµœì¢… ì‹¤íŒ¨ {len(still_failed)}ê°œ"
        )

        if still_failed:
            logger.info(
                f"  âŒ ìµœì¢… ì‹¤íŒ¨ í‹°ì»¤: {', '.join(still_failed[:10])}{'...' if len(still_failed) > 10 else ''}"
            )

        failed_tickers = still_failed

    fail_count = len(failed_tickers)

    # 3. ê²°ê³¼ ì €ì¥
    logger.info(f"\n{'=' * 60}")
    logger.info(f"ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {fail_count}ê°œ")

    # CSV/JSON ë°±ì—… ì €ì¥
    save_to_csv(all_data)

    # Supabase ì €ì¥ (ì˜µì…˜)
    if os.getenv("SUPABASE_URL") and os.getenv("SUPABASE_KEY"):
        db_count = save_to_supabase(all_data)
        logger.info(f"ğŸ—„ï¸ Supabase ì €ì¥: {db_count}ê°œ ê¸°ì—…")
    else:
        logger.info("âš ï¸ Supabase ì„¤ì • ì—†ìŒ - CSV/JSONë§Œ ì €ì¥ë¨")

    end_time = datetime.now(kst)
    duration = (end_time - start_time).total_seconds()

    logger.info(f"\nâ±ï¸ ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
    logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {end_time.strftime('%Y-%m-%d %H:%M:%S KST')}")
    logger.info("=" * 60)

    # Slack ì•Œë¦¼ ë°œì†¡ (ì œê±°ë¨)
    # if fail_count == 0:
    #     send_slack_notification(...)
    # else:
    #     send_slack_notification(...)


def run_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
    try:
        from apscheduler.schedulers.blocking import BlockingScheduler
        from apscheduler.triggers.cron import CronTrigger
    except ImportError:
        logger.error("âŒ APSchedulerê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        logger.error("   ì„¤ì¹˜: pip install APScheduler")
        return

    kst = pytz.timezone("Asia/Seoul")
    scheduler = BlockingScheduler(timezone=kst)

    # ë§¤ì¼ ìƒˆë²½ 5ì‹œ(KST) ì‹¤í–‰
    scheduler.add_job(
        collect_sp500_data,
        CronTrigger(hour=5, minute=0, timezone=kst),
        id="sp500_daily_collection",
        name="S&P 500 Daily Data Collection",
        replace_existing=True,
    )

    logger.info("=" * 60)
    logger.info("ğŸ“… S&P 500 ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")
    logger.info("   ì‹¤í–‰ ì‹œê°„: ë§¤ì¼ 05:00 KST")
    logger.info("   ì¢…ë£Œ: Ctrl+C")
    logger.info("=" * 60)

    # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ í‘œì‹œ
    job = scheduler.get_job("sp500_daily_collection")
    try:
        # APScheduler 3.x í˜¸í™˜
        next_run_time = getattr(job, "next_run_time", None) or getattr(
            job, "next_run", None
        )
        if next_run_time:
            logger.info(
                f"\nâ° ë‹¤ìŒ ì‹¤í–‰: {next_run_time.strftime('%Y-%m-%d %H:%M:%S %Z')}"
            )
        else:
            logger.info("\nâ° ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ í›„ ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ì´ ì„¤ì •ë©ë‹ˆë‹¤.")
    except Exception:
        logger.info("\nâ° ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")

    try:
        scheduler.start()
    except (KeyboardInterrupt, SystemExit):
        logger.info("\nğŸ›‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œë¨")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="S&P 500 ê¸°ì—… ì •ë³´ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python scripts/sp500_scheduler.py          # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ë§¤ì¼ 05:00 KST)
  python scripts/sp500_scheduler.py --test   # ì¦‰ì‹œ 1íšŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        """,
    )

    parser.add_argument(
        "--test", "-t", action="store_true", help="ì¦‰ì‹œ 1íšŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ìŠ¤ì¼€ì¤„ëŸ¬ ì—†ì´)"
    )

    args = parser.parse_args()

    if args.test:
        logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì¦‰ì‹œ ì‹¤í–‰")
        collect_sp500_data()
    else:
        run_scheduler()


if __name__ == "__main__":
    main()
